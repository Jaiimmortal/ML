{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAQ \n",
    "\n",
    "#### Topics covered : ML | DL | AI | Math and Concepts behind ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What's the difference between Loss function, Cost function and Objective function?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function** : Used to measure the *penalty*, of a certain data point, i.e., prediction or label.\n",
    "                    \n",
    "                    Example : Square loss and hinge loss.\n",
    "\n",
    "** Cost function ** : Used to compute the *sum of loss function* , and may have extra penalty set for the model\n",
    "                      such as *regularization*\n",
    "                      \n",
    "                    Example : Mean Squared Error loss (MSE) or Root Mean squared error loss (RMSE).\n",
    "                \n",
    "** Objective function** : A function which we are trying to optimize while training. Not necessarily a loss or cost\n",
    "                          function.\n",
    "                          \n",
    "                    Example : Maximum Likelihood Estimation (MLE), where we try to maximize.\n",
    "                    \n",
    "                    \n",
    "**TL;DR version** : A loss function **is a part of** a cost function **which is a type of** an objective function.\n",
    "                    However, the cost function is used more in optimization problem and loss function is used in parameter estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# References used : https://stats.stackexchange.com/a/179027/175877"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Why is derivative of an Activation function required in backpropagation?\n",
    "\n",
    "            in \n",
    "            new weight = old weight + learning rate * delta * df(e)/de * input\n",
    "\n",
    "            where `df(e)/de` is the derivative of activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the question *how should the weights be adjusted. i.e.,  which direction +/-? And by how much?*, also tells you *if you are going in the right direction to reach the function minimum.*\n",
    "\n",
    "Of course this arises other questions, like \n",
    "\n",
    "* the derivative of a sigmoid function is never negative as it seems, how can a weight decrease?\n",
    "* why would we need to find the minima of the loss function, but the formula isn't using that, it's actually using the activation function?\n",
    "\n",
    "Those are yet to be answered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# References used : https://stackoverflow.com/q/9785754/6905674"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. What's the difference between Prediction and Inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, in a real estate setting, one may seek to relate values of homes to inputs such as crime rate, zoning, distance from a river, air quality, schools, income level of community, size of houses, and so forth. In this case one might be interested in how the individual input variables affect the pricesâ€”that is, *how much extra will a house be worth if it has a view of the river?* This is a **inference problem.** Alternatively, one may simply be interested in predicting the value of a home given its characteristics: *is this house under- or over-valued?* This is a **prediction problem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# References used : Introduction to Statistical Learning pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Why are activation functions even used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To introduce *non linearity into the neural networks*, and why would we want it? Not all data is linear, and the weighted sum of inputs to the NN, to make them non-linear we use activation functions such as sigmoid, softmax, tanh etc.\n",
    "\n",
    "If not used, though they maybe multiple hidden layers (DNN), there is no use for those hidden layers, it's equivalent to one hidden layer. As sum of linear functions is equal to another linear function. So it's necessary to use an activation function over a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# References used : https://stackoverflow.com/q/9782071/6905674 \n",
    "#                   https://en.wikipedia.org/wiki/Activation_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. What's One-hot notation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce output, in the format which is understood in the form of digital network, where 0 means OFF and 1 means ON.\n",
    "\n",
    "In case we want to distinguish between Car, bike and truck.\n",
    "\n",
    "We can denote them as :\n",
    "\n",
    "* Car   = [1 0 0]'\n",
    "* Bike  = [0 1 0]'\n",
    "* Truck = [0 0 1]'\n",
    "\n",
    "Those all are vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. How does one choose which activation and cost functions to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works most of the times:\n",
    "\n",
    "1. If you do classification, use `softmax` for the last layer's nonlinearity and `cross entropy` as a cost\n",
    "   function.\n",
    "2. If you do regression, use `sigmoid` or `tanh` for the last layer's nonlinearity and `squared error` as a cost\n",
    "   function.\n",
    "3. Use `ReLU` as a nonlienearity between layers.\n",
    "4. Use better optimizers (`AdamOptimizer`, `AdagradOptimizer`) instead of `GradientDescentOptimizer`, or use momentum for faster convergence.\n",
    "\n",
    "Pls don't ask why. yet. Working on it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
